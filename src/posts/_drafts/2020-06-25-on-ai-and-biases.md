---
title: "On AI and ethical issues"
date: 2020-06-15
tags: [ai]
published: false
---

# Introduction
---

[[snippet]]
"AI" missuses are rampant - some are at best merely cringeworthy, others might have disturbing ramifications. In this post, I will try to reason and outline some of the issues caused by the lack of understanding on AI and arguments surrounding them.

# The Tech Mindset
---

Personally, I myself take on the software developer role because there is tremendous satisfaction in _being empowered_ in your job. Software developers are mini-gods of the machines - you have absolute control over software behavior. As long as something is possible, curiousity makes you feel like you have to try it. That's actually a desired quality in a good software developer - and also the reason for why we all have a ton of abandoned projects in the name of chasing shinier ideas.

This trait is not unique to tech either, it can be found in academia and other creative jobs. However, one thing separates software developers from other fields is that it's _extremely easy_ to turn a toy project, a prototype, into a fully-fledged product. It's the entire basis of Silicon Valley - the new American Dream: theoretically, it's possible to, with very few resources, start a one-man software product company. If not, it's easy to just build a product and throw it to the wild in the span of a weekend.

# The Business Mindset
---

Software helps businesses _scale_. It is natural for business owners to want to replace human resources, whenever possible, with software. This allows for more employee satisfaction and most importantly, helps the business behave consistently and predictably. Human errors are a real business risk - and there is no guarantee fixes (by means of policies and processes), compared to software where you can be sure that instructions will be followed to the letter.

However, it is also well known that "AI" - in the common sense of the word - is _not_ concrete instructions. I met a lot of cautious people who would ask how can you guarantee that the AI will work perfectly - and that's a major issue that the entire field is attempting to workaround (model interpretability - learning how a model comes to a particular decision). In the majority of AI projects I have encountered, the acceptance criteria will universally be a particular "(random 99-ish)% accuracy" - which sometimes is not even the accurate metrics for the task.

In general, however, most businesses are cautious in using AI, thanks to ownership of responsibility. Businesses can blame bad employees, sometimes even if such employee behavior stemmed from bad policies. When software goes rogue, the responsibility falls on _the business_ as a single entity. Most businesses I see avoid using AI for _critical_ functions, though they may experiment on the less-critical tasks - defined by reputation or money at risk.

# Where does it go wrong?
---

First, dataset bias is prevalent. Public datasets are mostly meant for research, as proof of concept for models - they often come with disclaimers on how they should not be used to build products.

# Looking forward
---
