---
title: A history of progress on text representation in NLP (Part 3 - Transformer models)
date: 2020-05-29
tags: [ai]
published: false
---

# Introduction
---

[[snippet]]
|This part is, in fact, a continuation of the previous part - still about sentence-level representations. However, I will discuss on what has been the state-of-the-art architecture for the last few years - Transformers.

Transformers are a class of sequential models based on the attention mechanism. In practice, they would stand in place of the LSTM layers that was the point of focus in [part 2](/2020-06-06-brief-history-of-nlp-p2).

# Transformer models
---

# 1. Attention Is All You Need

Prior to this, _attention mechanism_ has been a major breakthrough for machine translation (which are still RNN-based). As such, there were also various attempts to apply attention mechanism to RNN networks to various degrees of success. 

# 2. GPT

# 3. BERT

# Looking forward
---