{"componentChunkName":"component---src-pages-index-tsx","path":"/","result":{"data":{"allMarkdownRemark":{"edges":[{"node":{"snippet":"<div class=\"custom-block-body\"><p>In this tutorial I will briefly walk through how you can create an OpenAI Gym environment for the Google Chrome Dino game, and use Stable Baselines to quickly train an agent for it. Bonus: The environment can be easily extended to other browser games as well!.</p></div>","frontmatter":{"title":"Tutorial: Build AI to play Google Chrome Dino game with Reinforcement Learning in 30 minutes","date":"2020-06-15T00:00:00.000Z"},"fields":{"readingTime":{"text":"9 min read"},"slug":"/blog/2020-06-15-chrome-dino-game-reinforcement-learning/"}}},{"node":{"snippet":"<div class=\"custom-block-body\"><p>This part is, in fact, a continuation of the previous part - still about sentence-level representations. However, I will discuss on what has been the state-of-the-art architecture for the last few years - Transformers.</p></div>","frontmatter":{"title":"A history of progress on text representation in NLP (Part 3 - Transformer models)","date":"2020-06-11T00:00:00.000Z"},"fields":{"readingTime":{"text":"9 min read"},"slug":"/blog/2020-06-11-brief-history-of-nlp-p3/"}}},{"node":{"snippet":"<div class=\"custom-block-body\"><p>In the last part I talked about various ways of representing word as vectors in NLP. However, most state-of-the-art models (as of 2020 anyway) do not even care about word vectors at all! So what happened?</p></div>","frontmatter":{"title":"A history of progress on text representation in NLP (Part 2 - Sentence-level representation)","date":"2020-06-06T00:00:00.000Z"},"fields":{"readingTime":{"text":"12 min read"},"slug":"/blog/2020-06-06-brief-history-of-nlp-p2/"}}}]}},"pageContext":{}}}