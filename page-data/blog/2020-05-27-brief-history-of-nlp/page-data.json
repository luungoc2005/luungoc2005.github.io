{"componentChunkName":"component---src-components-blog-post-blog-post-tsx","path":"/blog/2020-05-27-brief-history-of-nlp/","result":{"data":{"markdownRemark":{"html":"<h1>Introduction</h1>\n<hr>\n<p>To start this off with a bit of personal background story: As far as formal education goes - I am no expert on AI. However that I managed to pick up and rapidly learn AI over the years - is due to the creative - but - intuitive nature of the ideas that are regularly churned out and experimented with, leading to state-of-the-art results.</p>\n<div class=\"custom-block snippet\"><div class=\"custom-block-body\"><p>In this article, I will outline through some of the most popular breakthroughs in AI in a semi-chronological order, with hope to inspire people on diving in deeper and experiment more, maybe even revisit old techniques and apply on top of newer architectures.</p></div></div>\n<p>I will skim on a lot of the intricacies and only focus on making a lot of this easy to understand and focus on the more intuitive parts. I highly encourage people to not be intimidated and read the original papers, with a lot of them detailing the process that lead the authors to the idea - that in itself is often extremely insightful and intriguing to read.</p>\n<h1>Central theme of ideas</h1>\n<hr>\n<p>About data representation in NLP: or - \"vectorization\" of text: In Computer Vision, this is often done through a ImageNet - trained model (a story for another time), with self-supervised learning a relatively recent art. </p>\n<p>For text, however, self-supervised learning has always been a high priority effort - due to the lack of <em>labelled</em> data. You will find a lot of ideas center around the creative use and manipulation of data to include extra data into the representation and reduce information loss. I will try to highlight this factor on every paper outlined here.</p>\n<h1>Progress on word representation</h1>\n<hr>\n<h1>1. One-hot encoding and TF-IDF</h1>\n<p>This is among the first and most naive way to apply machine learning for text.</p>\n<h3><em>One-hot Encoding</em>:</h3>\n<p>Suppose you have a dictionary of 5 words <code class=\"language-python\"><span class=\"token punctuation\">[</span><span class=\"token string\">\"I\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"hello\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"name\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"is\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"my\"</span><span class=\"token punctuation\">]</span></code></p>\n<p>The word \"I\" would be encoded as <code class=\"language-python\"><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span></code></p>\n<p>In practice, a vocabulary can go upwards to billions of words, which would easily overload storage with this method. To work around this problem, the <em>hashing trick</em> is often used, where words would be put in a fixed number of <em>buckets</em>, and possibly have overlapping buckets - but as long as the number of buckets is sufficiently large, there's still a good estimation on word distinction.</p>\n<p>Remarks: This is essentially a way to mathematically represent a program that would look in the text for specific mentions of certain words.</p>\n<h3><em>TF-IDF</em>:</h3>\n<p>One problem from one-hot encoding representation is that different words are treated similarly. A lot of times words such as \"is\", \"my\", \"the\" can appear in all sentences, therefore would provide faulty signal (as in, you don't want your model to use the word \"the\" as an indicator of 2 sentences being of different intents).</p>\n<p>One way to address this problem is to remove all words in a list of <em>stop words</em></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> nltk<span class=\"token punctuation\">.</span>corpus <span class=\"token keyword\">import</span> stopwords\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>stopwords<span class=\"token punctuation\">.</span>words<span class=\"token punctuation\">(</span><span class=\"token string\">'english'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>TF-IDF attempts to, instead, count the number of word occurrences, downplaying the more common words and putting more weight on the less common words (stronger signal). This is a step up from one-hot encoding, but is still relatively naive. We will progress extremely fast from here.</p>\n<p>Reference example of these methods: </p>\n<ul>\n<li><a href=\"https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html\">Classification of text documents using sparse features (from scikit-learn documentation)</a></li>\n</ul>\n<h1>2. Word2Vec</h1>\n<p>This paper was a milestone for the use of deep neural networks in NLP. It proposed 2 self-supervised tasks, called <em>\"skip-gram\"</em> and <em>\"continuous bags of words\" (CBOW)</em>, briefly described below:</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Skip-gram</th>\n<th align=\"center\">CBOW</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">Predict surrounding words</td>\n<td align=\"center\">Predict middle word</td>\n</tr>\n<tr>\n<td align=\"center\"><code class=\"language-text\">(&lt;?>,&lt;?>,brown,&lt;?>,&lt;?>) ➞ (the,quick,fox,jumps)</code></td>\n<td align=\"center\"><code class=\"language-text\">(the,quick,&lt;?>,fox, jumps) ➞ brown</code></td>\n</tr>\n</tbody>\n</table>\n<p>With <em>gradient descent</em>, it begins by randomizing - usually a 200-d or 300-d vector - for each words (instead of one-hot encoding), then train for this objective. By the end of training, the resulting word vectors gain an interesting property: the words that are closer in meaning are generally <em>drawn closer</em> - meaning having shorter vector distance. One way to understand this intuitively is similar words tend to be interchangeable in a sentence.\n<figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 992px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/de278efc2f2441f4083a7c682e75213f/42a07/brief-history-of-nlp-embedding.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 81.85483870967741%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAQABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAIDBAX/xAAVAQEBAAAAAAAAAAAAAAAAAAAAAf/aAAwDAQACEAMQAAAB3UWNbQCf/8QAGhAAAgMBAQAAAAAAAAAAAAAAAAECERIDMf/aAAgBAQABBQLw0rOkdRUVZ//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABsQAAICAwEAAAAAAAAAAAAAAAERABAhMUFh/9oACAEBAAY/AonmuuAnftf/xAAZEAEBAQEBAQAAAAAAAAAAAAABEQBBEDH/2gAIAQEAAT8hZlxAXNL1zrGHxHEjnPXn/9oADAMBAAIAAwAAABDYz//EABYRAQEBAAAAAAAAAAAAAAAAAAEQQf/aAAgBAwEBPxADZ//EABYRAQEBAAAAAAAAAAAAAAAAACEQEf/aAAgBAgEBPxDAn//EAB4QAQACAQQDAAAAAAAAAAAAAAEAESExQVFhkaHw/9oACAEBAAE/EGSodtQJckNyTc9kATAtU3XOnmGIkqwyrIcusDHxP//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Projecting the vectors into 3D space\"\n        title=\"Projecting the vectors into 3D space\"\n        src=\"/static/de278efc2f2441f4083a7c682e75213f/0c20b/brief-history-of-nlp-embedding.jpg\"\n        srcset=\"/static/de278efc2f2441f4083a7c682e75213f/2edd3/brief-history-of-nlp-embedding.jpg 248w,\n/static/de278efc2f2441f4083a7c682e75213f/6a372/brief-history-of-nlp-embedding.jpg 496w,\n/static/de278efc2f2441f4083a7c682e75213f/0c20b/brief-history-of-nlp-embedding.jpg 992w,\n/static/de278efc2f2441f4083a7c682e75213f/42a07/brief-history-of-nlp-embedding.jpg 1450w\"\n        sizes=\"(max-width: 992px) 100vw, 992px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\"><p>Projecting the vectors into 3D space</p></figcaption>\n  </figure></p>\n<p>You can try the <a href=\"http://projector.tensorflow.org/\">Tensorboard Embedding Projector</a> to see what this looks like.</p>\n<p>Another property is that the word vectors can represent some aspects of the meaning, often demonstrated by that we can apply this kind of formula with the resulting word vectors:</p>\n<p><code class=\"language-text\">king - man + woman ~ queen</code> (gender)</p>\n<p>or</p>\n<p><code class=\"language-text\">paris - france + poland ~ warsaw</code> (capital city)</p>\n<p>Naturally this very cherry-picked and is mainly for illustrative purpose and actually doesn't have many applications, but is interesting nontheless.</p>\n<h3>GloVe and FastText</h3>\n<p>GloVe is actually a entirely different formulation altogether (using co-occurence of words) to achieve the same result of <em>drawing similar words closer together</em> in vector space. This resulted in, arguably, the state of the art word vectors for a very long time.</p>\n<p>FastText is a more recent variation of the original Word2Vec formulation: Using the same training objectives, but more performance-optimized training code and more importantly, utilizing n-gram subword features. Specifically, for a 3-gram model, it would break down a word as follows:</p>\n<p><code class=\"language-text\">where ➞ (&lt;wh, whe, her, ere, re>, &lt;where>)</code>\n(note the <code class=\"language-text\">&lt;</code> and <code class=\"language-text\">></code> markers to denote the beggining and end of the word)</p>\n<p>The final word vector for <code class=\"language-text\">where</code> is then computed as the sum of the modular vectors.</p>\n<p>This method has 2 advantages over the original paper: first, including subword information is helpful, at least for the English language - as e.g <code class=\"language-text\">national</code> and <code class=\"language-text\">nationalism</code> would share a lot of aspects in meaning. Second - this helps dealing with unseen or made-up words, by being able to approximate the meaning if it's based on existing words.</p>\n<p>In practice, FastText results in relatively comparable performance with GloVe, but has the ability to account for unseen words.</p>\n<p>Why I specifically dived into FastText is because <em>breaking down words into subword units</em> would later become a staple for language models to limit vocabulary, which improves computational efficiency, and giving the ability to account for unseen words.</p>\n<p>References:</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1301.3781\">Efficient Estimation of Word Representations in Vector Space (2013)</a></li>\n<li><a href=\"https://nlp.stanford.edu/projects/glove/\">GloVe: Global Vectors for Word Representation</a></li>\n<li><a href=\"https://arxiv.org/abs/1607.04606\">Enriching Word Vectors with Subword Information</a></li>\n</ul>\n<h1>3. CoVe and ELMo</h1>\n<p>These papers are actually released much later than the initial Word2Vec era, and built upon ideas and achievements from <em>sentence representation</em>. I initially debated to include this in the 2nd part of the article, though this can be used on top of sentence representation in a modular fashion qualifies it for this part.</p>\n<p>Initially, CoVe was introduced as a way to take contextual meaning into account for word vectors, through a machine translation task. It uses GloVe vectors, put through a LSTM-based machine translation model, and keep the resulting encoder (discarding the decoder). Note that the encoder is bidirectional (this will be relevant later). The resulting output of the encoder can be concatenated with the input GloVe vectors and used as input for downstream tasks.</p>\n<p>The practical achivement of CoVe is that it can be used as an addition to all existing Word2Vec-based models and immediately gives a boost in performance. This quantifies how much performance can be gained from this feature. It also introduces using an additional character-level CNN model for adding character-level features, which it also proves to be able to contribute additional performance at a computational cost. There was also one problem with CoVe, is that it's a supervised training objective - requiring language translation data, which might not be available in a large amount for a lot of languages.</p>\n<p>The improvements from CoVe were quickly forgotten, however, after ELMo was released. ELMo is a forward and backward language model concatenated together (note that this is not <em>true</em> bidirectional) that uses character-level CNN features, trained on the <em>1 Billion Word Language Model Benchmark</em> dataset from scratch. It made a big splash when it was released because:</p>\n<ul>\n<li>It could be used on top of all existing models as a replacement for word embeddings</li>\n<li>It managed to both employ character-level features (also giving it the ability to handle out-of-vocabulary words) and contextual information for words</li>\n<li>It improved state-of-the-art by a huge margin</li>\n</ul>\n<p>ELMo is, essentially, the pinnacle of the RNN era - where research was still focused on RNN language models. Transformers architecture actually quickly took the stage afterwards. In part 2 I will go into more details on what language models <em>are</em> and introduce Transformers-based models. Stay tuned!</p>\n<p>References:</p>\n<ul>\n<li><a href=\"http://papers.nips.cc/paper/7209-learned-in-translation-contextualized-word-vectors.pdf\">Learned in Translation: Contextualized Word Vectors</a></li>\n<li><a href=\"https://arxiv.org/pdf/1802.05365.pdf\">Deep contextualized word representations</a></li>\n</ul>","frontmatter":{"title":"A history of progress on text representation in NLP (Part 1 - Word-level representation)","date":"2020-05-27T00:00:00.000Z"},"fields":{"readingTime":{"text":"8 min read"}}}},"pageContext":{"slug":"/blog/2020-05-27-brief-history-of-nlp/"}},"staticQueryHashes":[]}