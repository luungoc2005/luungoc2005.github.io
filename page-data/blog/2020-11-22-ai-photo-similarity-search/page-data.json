{"componentChunkName":"component---src-components-blog-post-blog-post-tsx","path":"/blog/2020-11-22-ai-photo-similarity-search/","result":{"data":{"markdownRemark":{"html":"<h1>Introduction</h1>\n<hr>\n<div class=\"custom-block snippet\"><div class=\"custom-block-body\"><p>Have you ever wondered how you could build your own Google Image Search? This tutorial will walk you through so you can build your own in 15 minutes.</p></div></div>\n<h1>Idea: Image encodings</h1>\n<hr>\n<p>Often the most obvious kind of search engine to build would be to search for text. However, for all types of data that are not convertible to text - e.g Music, text-like searching is not always feasible.</p>\n<p>So how would you build a image search engine? a few ideas come to mind: To compare the pixels of the photos: this will only find exact same photos or with different lighting conditions. Or we can use <em>AI</em> to convert a photo into a text that describes the photo: this, for example, will struggle to differentiate between different kinds of cats, if it can only infer that the photo contains a cat but not <em>which</em> breed.</p>\n<p>Instead, if we can somehow convert a photo into a <em>vector</em>, we can measure the distance then rank all the distances and find the closest photos.</p>\n<p>We can do this by using almost any image neural network, however the most common, for good reasons, are ones trained on ImageNet.</p>\n<p>ImageNet models are models trained to classify the 1000 different objects in the ImageNet dataset. The last layer of an ImageNet model would be a 1000 dimension - classification logits layer. In order to take image encoding from a ImageNet model, we can take the outputs from the <em>second-to-last</em> layer instead, which would contain the information derived from all the upstream layer and are important for the classification.</p>\n<p>For illustration, I'm going to use MobileNet - one of the most lightweight ImageNet-trained models, along with Pytorch. For different models, you might need to use GPU to obtain reasonable performance.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">from</span> torchvision <span class=\"token keyword\">import</span> transforms\n<span class=\"token keyword\">from</span> PIL <span class=\"token keyword\">import</span> Image\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">encode_image</span><span class=\"token punctuation\">(</span>file_path<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\"># Taken from https://pytorch.org/hub/pytorch_vision_mobilenet_v2/</span>\n    transform_test <span class=\"token operator\">=</span> transforms<span class=\"token punctuation\">.</span>Compose<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n        transforms<span class=\"token punctuation\">.</span>Resize<span class=\"token punctuation\">(</span><span class=\"token number\">256</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n        transforms<span class=\"token punctuation\">.</span>CenterCrop<span class=\"token punctuation\">(</span><span class=\"token number\">224</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n        transforms<span class=\"token punctuation\">.</span>ToTensor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n        transforms<span class=\"token punctuation\">.</span>Normalize<span class=\"token punctuation\">(</span>mean<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token number\">0.485</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0.456</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0.406</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> std<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token number\">0.229</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0.224</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0.225</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">class</span> <span class=\"token class-name\">Flatten</span><span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n        <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            <span class=\"token comment\"># Reshapes the input into a 2d array of (batch_size, n)</span>\n            <span class=\"token keyword\">return</span> x<span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n\n    model <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>hub<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span><span class=\"token string\">'pytorch/vision:v0.6.0'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'mobilenet_v2'</span><span class=\"token punctuation\">,</span> pretrained<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># replace the classifier layer with the Flatten() layer</span>\n    model<span class=\"token punctuation\">.</span>classifier <span class=\"token operator\">=</span> Flatten<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span><span class=\"token builtin\">eval</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span>file_path<span class=\"token punctuation\">,</span> <span class=\"token string\">'rb'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> fp<span class=\"token punctuation\">:</span>\n        img <span class=\"token operator\">=</span> Image<span class=\"token punctuation\">.</span><span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span>fp<span class=\"token punctuation\">)</span>\n    input_tensor <span class=\"token operator\">=</span> transform_test<span class=\"token punctuation\">(</span>img<span class=\"token punctuation\">)</span>\n    input_batch <span class=\"token operator\">=</span> input_tensor<span class=\"token punctuation\">.</span>unsqueeze<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">with</span> torch<span class=\"token punctuation\">.</span>no_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        output <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>input_batch<span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\n\n    <span class=\"token keyword\">return</span> output<span class=\"token punctuation\">.</span>cpu<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>numpy<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>This is a powerful concept, and basis for many AI-based search: This is especially true for Encoder-Decoder (e.g Seq2Seq) kinds of architecture - where there is a \"bottleneck\" layer, you can use the vector output of that layer to use a compressed the representation of the input. Therefore this can also apply to text semantic searching or face recognition (i.e by using a face encoder model).</p>\n<h1>Idea: Vector similarity</h1>\n<hr>\n<p>The output of the above <code class=\"language-text\">encode_image()</code> function will be a vector - so how would you compare between vectors? that would be by calculating distance. One straightforward way to do this is by:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\ndistance <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>linalg<span class=\"token punctuation\">.</span>norm<span class=\"token punctuation\">(</span>vector1 <span class=\"token operator\">-</span> vector2<span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>You can then call <code class=\"language-text\">encode_image()</code> for every image in your library to \"index\" it. You can then encode any given photo and compare to look for the desired photo. It might take some experimentation to find out a <strong>threshold</strong> for a \"match\" result - otherwise you can only rank the results - from most similar to least.</p>\n<p>For example, for these photos:</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Photo 1</th>\n<th align=\"center\">Photo 2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\"><figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 992px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/dd0a675c5a0fc3f562510567e48f40f7/15ffe/photo-similarity-dog.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 78.62903225806453%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAQABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAQFA//EABUBAQEAAAAAAAAAAAAAAAAAAAEC/9oADAMBAAIQAxAAAAHJRmiEEZJP/8QAGRAAAwEBAQAAAAAAAAAAAAAAAQIDAAQS/9oACAEBAAEFAuWCstIlH8aNSmo0aBwA3//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABwQAAICAgMAAAAAAAAAAAAAAAABERICECJhof/aAAgBAQAGPwJ3Uz4Vy10K7OLlH//EABoQAQADAQEBAAAAAAAAAAAAAAEAEVExIWH/2gAIAQEAAT8hdRHkb9hMHOOkprLEb+Iv0pnYlMBn/9oADAMBAAIAAwAAABDzz//EABYRAQEBAAAAAAAAAAAAAAAAABEAAf/aAAgBAwEBPxBcnb//xAAWEQEBAQAAAAAAAAAAAAAAAAABABH/2gAIAQIBAT8Q3SVv/8QAGxABAAIDAQEAAAAAAAAAAAAAAQAhETFBYXH/2gAIAQEAAT8Q3uICh5LXS2fBIFQUtizEB3NxWi0MtOj5PdTmyf/Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Dog\"\n        title=\"Dog\"\n        src=\"/static/dd0a675c5a0fc3f562510567e48f40f7/0c20b/photo-similarity-dog.jpg\"\n        srcset=\"/static/dd0a675c5a0fc3f562510567e48f40f7/2edd3/photo-similarity-dog.jpg 248w,\n/static/dd0a675c5a0fc3f562510567e48f40f7/6a372/photo-similarity-dog.jpg 496w,\n/static/dd0a675c5a0fc3f562510567e48f40f7/0c20b/photo-similarity-dog.jpg 992w,\n/static/dd0a675c5a0fc3f562510567e48f40f7/09f78/photo-similarity-dog.jpg 1488w,\n/static/dd0a675c5a0fc3f562510567e48f40f7/15ffe/photo-similarity-dog.jpg 1546w\"\n        sizes=\"(max-width: 992px) 100vw, 992px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\"><p>Dog</p></figcaption>\n  </figure></td>\n<td align=\"center\"><figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 992px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/f7bcdd3a95d360183130bf3f8a40bb3b/72e01/photo-similarity-cat.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.451612903225815%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAQFAv/EABYBAQEBAAAAAAAAAAAAAAAAAAIAAf/aAAwDAQACEAMQAAABYUrTwsjpZ//EABoQAAMAAwEAAAAAAAAAAAAAAAABAhESITH/2gAIAQEAAQUCcOZmzXI+pRKF5//EABURAQEAAAAAAAAAAAAAAAAAAAAR/9oACAEDAQE/AUf/xAAVEQEBAAAAAAAAAAAAAAAAAAAAEf/aAAgBAgEBPwFX/8QAGhAAAgIDAAAAAAAAAAAAAAAAAAERIQIQMf/aAAgBAQAGPwJuSGyskWc1/8QAGRABAAMBAQAAAAAAAAAAAAAAAQARIUGR/9oACAEBAAE/IaAMIN63mTukgM1xCzUBweT/2gAMAwEAAgADAAAAECP/AP/EABcRAQADAAAAAAAAAAAAAAAAAAEQESH/2gAIAQMBAT8QDKh//8QAFxEAAwEAAAAAAAAAAAAAAAAAAAFBcf/aAAgBAgEBPxBumD//xAAaEAEBAQEAAwAAAAAAAAAAAAABEQAhMUFh/9oACAEBAAE/EHqvaD3WKxBVM09UUrnYQTtPOYlA8cQTt8b/2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Cat\"\n        title=\"Cat\"\n        src=\"/static/f7bcdd3a95d360183130bf3f8a40bb3b/0c20b/photo-similarity-cat.jpg\"\n        srcset=\"/static/f7bcdd3a95d360183130bf3f8a40bb3b/2edd3/photo-similarity-cat.jpg 248w,\n/static/f7bcdd3a95d360183130bf3f8a40bb3b/6a372/photo-similarity-cat.jpg 496w,\n/static/f7bcdd3a95d360183130bf3f8a40bb3b/0c20b/photo-similarity-cat.jpg 992w,\n/static/f7bcdd3a95d360183130bf3f8a40bb3b/72e01/photo-similarity-cat.jpg 1024w\"\n        sizes=\"(max-width: 992px) 100vw, 992px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\"><p>Cat</p></figcaption>\n  </figure></td>\n</tr>\n</tbody>\n</table>\n<p>distance = 24.56893</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Photo 1</th>\n<th align=\"center\">Photo 2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\"><figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 992px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/f7bcdd3a95d360183130bf3f8a40bb3b/72e01/photo-similarity-cat.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.451612903225815%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAQFAv/EABYBAQEBAAAAAAAAAAAAAAAAAAIAAf/aAAwDAQACEAMQAAABYUrTwsjpZ//EABoQAAMAAwEAAAAAAAAAAAAAAAABAhESITH/2gAIAQEAAQUCcOZmzXI+pRKF5//EABURAQEAAAAAAAAAAAAAAAAAAAAR/9oACAEDAQE/AUf/xAAVEQEBAAAAAAAAAAAAAAAAAAAAEf/aAAgBAgEBPwFX/8QAGhAAAgIDAAAAAAAAAAAAAAAAAAERIQIQMf/aAAgBAQAGPwJuSGyskWc1/8QAGRABAAMBAQAAAAAAAAAAAAAAAQARIUGR/9oACAEBAAE/IaAMIN63mTukgM1xCzUBweT/2gAMAwEAAgADAAAAECP/AP/EABcRAQADAAAAAAAAAAAAAAAAAAEQESH/2gAIAQMBAT8QDKh//8QAFxEAAwEAAAAAAAAAAAAAAAAAAAFBcf/aAAgBAgEBPxBumD//xAAaEAEBAQEAAwAAAAAAAAAAAAABEQAhMUFh/9oACAEBAAE/EHqvaD3WKxBVM09UUrnYQTtPOYlA8cQTt8b/2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Cat\"\n        title=\"Cat\"\n        src=\"/static/f7bcdd3a95d360183130bf3f8a40bb3b/0c20b/photo-similarity-cat.jpg\"\n        srcset=\"/static/f7bcdd3a95d360183130bf3f8a40bb3b/2edd3/photo-similarity-cat.jpg 248w,\n/static/f7bcdd3a95d360183130bf3f8a40bb3b/6a372/photo-similarity-cat.jpg 496w,\n/static/f7bcdd3a95d360183130bf3f8a40bb3b/0c20b/photo-similarity-cat.jpg 992w,\n/static/f7bcdd3a95d360183130bf3f8a40bb3b/72e01/photo-similarity-cat.jpg 1024w\"\n        sizes=\"(max-width: 992px) 100vw, 992px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\"><p>Cat</p></figcaption>\n  </figure></td>\n<td align=\"center\"><img src=\"/assets/photo-similarity-cat2.jpg\" alt=\"Cat\"></td>\n</tr>\n</tbody>\n</table>\n<p>distance = 16.510061</p>\n<p>For the example above, a threshold that would be able to differentiate between dogs and cats, but can find different spieces of dogs or cats, would lie between ~16-24</p>\n<p>This is but one crude method of searching by measuring vector similarity. If you were to use this, you would need a loop through all the vector-encoded images and calculate the distance between the input photo and each target photo.</p>\n<p>One can use several different methods to scale up vector similarity-based search, which I will write about in a separate article.</p>","frontmatter":{"title":"How to build a simple AI photo search engine","date":"2020-11-22T00:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"}}}},"pageContext":{"slug":"/blog/2020-11-22-ai-photo-similarity-search/"}}}