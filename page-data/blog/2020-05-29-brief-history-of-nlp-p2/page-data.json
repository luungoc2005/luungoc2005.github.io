{"componentChunkName":"component---src-components-blog-post-blog-post-tsx","path":"/blog/2020-05-29-brief-history-of-nlp-p2/","result":{"data":{"markdownRemark":{"html":"<h1>Introduction</h1>\n<hr>\n<div class=\"custom-block snippet\"><div class=\"custom-block-body\"><p>In the last part I talked about various ways of representing word as vectors in NLP. However, most state-of-the-art models (as of 2020 anyway) do not even care about word vectors at all! So what happened?</p></div></div>\n<p>It should be obvious that using word vectors leave out an important aspect of all languages: word context. Words should not be treated individually, because a single word can have multiple and vastly different meanings in different contexts - consider <code class=\"language-text\">content</code> in <code class=\"language-text\">table of contents</code> and <code class=\"language-text\">I am content with my job</code>.</p>\n<p>In the end, most applications of NLP would require using sentences or even paragraphs or documents level representations. Examining word vectors remain mostly useful for analyzing and validating linguistic properties. That's why this part will probably drag out way longer than the first.</p>\n<h1>Progress on sentence representation</h1>\n<hr>\n<h1>1. Bag of Words</h1>\n<p>The original bag of words model involve counting occurences of words and putting inside a fixed-size dictionary vector. This is one-hot encoding on the sentence level - as illustrated below:</p>\n<p>A dictionary such as:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token punctuation\">[</span><span class=\"token string\">\"the\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"quick\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"brown\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"fox\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"jumps\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"dog\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"like\"</span><span class=\"token punctuation\">]</span></code></pre></div>\n<p>Would represent the sentence <code class=\"language-text\">I like dog but I do not like fox</code> (forgive the grammar, it's beside the point) as:</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">the</th>\n<th align=\"center\">quick</th>\n<th align=\"center\">brown</th>\n<th align=\"center\">fox</th>\n<th align=\"center\">jumps</th>\n<th align=\"center\">dog</th>\n<th align=\"center\">like</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">1</td>\n<td align=\"center\">0</td>\n<td align=\"center\">1</td>\n<td align=\"center\">2</td>\n</tr>\n</tbody>\n</table>\n<p>Which is actually the sum of one-hot encoded word vectors!</p>\n<p>Using word vectors, the bag of words model is either the sum or the average of word vectors - not necessarily one-hot encoded vectors, but often the pre-trained Word2Vec, GloVe or FastText vectors I mentioned in <a href=\"/blog/2020-05-27-brief-history-of-nlp.md\">part 1</a>.</p>\n<p>As immediately evident, this method disregards contextual information and word ordering. Used for sentence/document classification, it would be a glorified checking substrings algorithm.</p>\n<h2>SIF and FastText</h2>\n<p>One attempt to improve upon this method is to use a weighted average instead. This can be considered a direct improvement over removing stop words - we can use TF-IDF weighting scheme to determine the weights of word vectors, or a improved version - <em>smooth inverse frequency</em> (SIF). This method is implemented in <em>A simple but tough-to-beat baseline for sentence embeddings</em> - which actually outperforms several more complex models that actually takes into account word ordering (*on select tasks) - some of which we will explore later on in this article.</p>\n<ul>\n<li>Implementation: <a href=\"https://github.com/PrincetonML/SIF\">PrincetonML /\nSIF (Github)</a></li>\n</ul>\n<p>If you tried to <em>google</em> for FastText, you might also have seen the paper <em>Bag of Tricks for Efficient Text Classification</em> . This paper actually employs a similar method to vanilla bag of words - to take a (normal) average of word vectors. The difference here is it also encodes n-grams of words as separate tokens, which gives it a form of word order information.</p>\n<p>For example, the following sentence:</p>\n<p><code class=\"language-text\">The quick brown fox jumps</code></p>\n<p>Can be broken into these tokens (assuming a 2-gram model):</p>\n<p><code class=\"language-text\">&lt;the&gt;, &lt;quick&gt;, &lt;brown&gt;, &lt;fox&gt;, &lt;jumps&gt;, &lt;the quick&gt;, &lt;quick brown&gt;, &lt;brown fox&gt;, &lt;fox jumps&gt;</code></p>\n<p>Then encoded and averaged into a sentence vector.</p>\n<ul>\n<li>Tutorial: <a href=\"https://fasttext.cc/docs/en/supervised-tutorial.html\">Text classification using FastText</a></li>\n<li>An approximate implementation that can give a good understanding on how FastText works lie in the <a href=\"https://keras.io/examples/imdb_fasttext/\">Keras examples</a>.</li>\n<li>Pytorch also has an implementation of this method in its <a href=\"https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\">examples documentation</a></li>\n</ul>\n<p>A honorable mention: this method bears homeage to <em>Convolutional Neural Networks for Sentence Classification</em>, where the use of convolutional network bears resemblance to this method of taking n-grams into account. Though empirically, this method of using CNNs might not necessarily outperform the various Bag-of-Words variations above, and is more computationally expensive.</p>\n<p>References:</p>\n<ul>\n<li><a href=\"https://openreview.net/pdf?id=SyK00v5xx\">A simple but tough-to-beat baseline for sentence embeddings</a></li>\n<li><a href=\"https://arxiv.org/pdf/1607.01759.pdf\">Bag of Tricks for Efficient Text Classification</a></li>\n<li><a href=\"https://www.aclweb.org/anthology/D14-1181.pdf\">Convolutional Neural Networks for Sentence Classification</a></li>\n</ul>\n<h1>2. Skip-Thought Vectors</h1>\n<p>Note that by this time - People are not longer strangers to using recurrent networks (GRU/LSTM) for machine translation. This is, to my knowledge, the first attempt on applying Machine Translation to self-supervised training. The paper also introduces <em>BookCorpus</em>, which is a dataset that is often used to this day in combination with Wikipedia for training language models.</p>\n<p>The training objective of Skip-Thought Vectors is to generate (or \"translate\") the previous and following sentence for any given sentence (similar to Word2Vec's skip-gram setup), as illustrated below:</p>\n<p><figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 881px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/32c4a4588d2910730aa54ad9bd4ce9a2/96658/brief-history-of-nlp-p2-skip-thought-objective.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 20.161290322580648%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAAAsSAAALEgHS3X78AAAAh0lEQVQY062Pyw7CIBQF+f9/LGArUF7GpLYxhXZEXLsw8SaTOTlndQV/vLMhaq3s+04ppfsbn71wzzfCNDGPI77ZKoXVmnEYuFmLWNcV7wMxps47p5QJITYncs6tj6333VJKxovGWYMxV7SSqMZjWXhuG+LXt0ot+OhxYSakgPMOM1uO8+j7Cwg9NUzWum1jAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Skip-Thought Vectors training objective\"\n        title=\"Skip-Thought Vectors training objective\"\n        src=\"/static/32c4a4588d2910730aa54ad9bd4ce9a2/96658/brief-history-of-nlp-p2-skip-thought-objective.png\"\n        srcset=\"/static/32c4a4588d2910730aa54ad9bd4ce9a2/544ba/brief-history-of-nlp-p2-skip-thought-objective.png 248w,\n/static/32c4a4588d2910730aa54ad9bd4ce9a2/bb630/brief-history-of-nlp-p2-skip-thought-objective.png 496w,\n/static/32c4a4588d2910730aa54ad9bd4ce9a2/96658/brief-history-of-nlp-p2-skip-thought-objective.png 881w\"\n        sizes=\"(max-width: 881px) 100vw, 881px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\"><p>Skip-Thought Vectors training objective</p></figcaption>\n  </figure></p>\n<p>Side note: This paper also introduces an <em>interesting</em> technique for handling unseen words: It trains a linear mapping between Word2Vec and its encoder's embeddings layer for shared words. After that, unseen words can be taken from Word2Vec and mapped to the model's embedding space. This does seem to work decently from the examples given by the paper.</p>\n<p>All in all, the 2 most important contributions of this work is the BookCorpus dataset and it paved the way for sentence embeddings by proving that it can outperform existing techniques.</p>","frontmatter":{"title":"A history of progress on text representation in NLP (Part 2 - Sentence-level representation)","date":"2020-05-29T00:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"}}}},"pageContext":{"slug":"/blog/2020-05-29-brief-history-of-nlp-p2/"}}}