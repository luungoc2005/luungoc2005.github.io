{"componentChunkName":"component---src-components-blog-post-blog-post-tsx","path":"/blog/2020-05-29-brief-history-of-nlp-p2/","result":{"data":{"markdownRemark":{"html":"<h1>Introduction</h1>\n<hr>\n<div class=\"custom-block snippet\"><div class=\"custom-block-body\"><p>In the last part I talked about various ways of representing word as vectors in NLP. However, most state-of-the-art models (as of 2020 anyway) do not even care about word vectors at all! So what happened?</p></div></div>\n<p>It should be obvious that using word vectors leave out an important aspect of all languages: word context. Words should not be treated individually, because a single word can have multiple and vastly different meanings in different contexts - consider <code class=\"language-text\">content</code> in <code class=\"language-text\">table of contents</code> and <code class=\"language-text\">I am content with my job</code>.</p>\n<p>In the end, most applications of NLP would require using sentences or even paragraphs or documents level representations. Examining word vectors remain mostly useful for analyzing and validating linguistic properties. That's why this part will probably drag out way longer than the first.</p>\n<h1>Progress on sentence representation</h1>\n<hr>\n<h1>1. Bag of Words</h1>\n<p>The original bag of words model involve counting occurences of words and putting inside a fixed-size dictionary vector. This is one-hot encoding on the sentence level - as illustrated below:</p>\n<p>A dictionary such as:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token punctuation\">[</span><span class=\"token string\">\"the\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"quick\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"brown\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"fox\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"jumps\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"dog\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"like\"</span><span class=\"token punctuation\">]</span></code></pre></div>\n<p>Would represent the sentence <code class=\"language-text\">I like dog but I do not like fox</code> (forgive the grammar, it's beside the point) as:</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">the</th>\n<th align=\"center\">quick</th>\n<th align=\"center\">brown</th>\n<th align=\"center\">fox</th>\n<th align=\"center\">jumps</th>\n<th align=\"center\">dog</th>\n<th align=\"center\">like</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">1</td>\n<td align=\"center\">0</td>\n<td align=\"center\">1</td>\n<td align=\"center\">2</td>\n</tr>\n</tbody>\n</table>\n<p>Which is actually the sum of one-hot encoded word vectors!</p>\n<p>Using word vectors, the bag of words model is either the sum or the average of word vectors.</p>\n<p>As immediately evident, this method disregards contextual information and word ordering. Used for sentence/document classification, it would be a glorified checking substrings algorithm.</p>","frontmatter":{"title":"A history of progress on text representation in NLP (Part 2 - Sentence-level representation)","date":"2020-05-27T00:00:00.000Z"},"fields":{"readingTime":{"text":"2 min read"}}}},"pageContext":{"slug":"/blog/2020-05-29-brief-history-of-nlp-p2/"}}}