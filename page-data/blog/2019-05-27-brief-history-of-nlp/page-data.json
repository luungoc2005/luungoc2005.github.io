{"componentChunkName":"component---src-components-blog-post-blog-post-tsx","path":"/blog/2019-05-27-brief-history-of-nlp/","result":{"data":{"markdownRemark":{"html":"<h1>Introduction</h1>\n<hr>\n<p>To start this off with a bit of personal background story: As far as formal education goes - I am no expert on AI. However that I managed to pick up and rapidly learn AI over the years - is due to the creative - but - intuitive nature of the ideas that are regularly churned out and experimented with, leading to state-of-the-art results.</p>\n<div class=\"custom-block snippet\"><div class=\"custom-block-body\"><p>In this article, I will outline through some of the most popular breakthroughs in AI - with hope to inspire people on diving in deeper and experiment more - maybe even revisit old techniques and apply on top of newer architectures.</p></div></div>\n<p>I will skim on a lot of the intricacies and only focus on making a lot of this easy to understand and focus on the more intuitive parts. I highly encourage people to not be intimidated and read the original papers, with a lot of them detailing the process that lead the authors to the idea - that in itself is often extremely insightful and intriguing to read.</p>\n<h1>Central theme of ideas</h1>\n<hr>\n<p>About data representation in NLP: or - \"vectorization\" of text: In Computer Vision, this is often done through a ImageNet - trained model (a story for another time), with self-supervised learning a relatively recent art. </p>\n<p>For text, however, self-supervised learning has always been a high priority effort - due to the lack of <em>labelled</em> data. You will find a lot of ideas center around the creative use and manipulation of data to include extra data into the representation and reduce information loss. I will try to highlight this factor on every paper outlined here.</p>\n<h1>Progress on text representation</h1>\n<hr>\n<h1>1. One-hot encoding and TF-IDF</h1>\n<p>This is among the first and most naive way to apply machine learning for text.</p>\n<h3><em>One-hot Encoding</em>:</h3>\n<p>Suppose you have a dictionary of 5 words <code class=\"language-python\"><span class=\"token punctuation\">[</span><span class=\"token string\">\"I\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"hello\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"name\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"is\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"my\"</span><span class=\"token punctuation\">]</span></code></p>\n<p>The word \"I\" would be encoded as <code class=\"language-python\"><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span></code></p>\n<p>In practice, a vocabulary can go upwards to billions of words, which would easily overload storage with this method. To work around this problem, the <em>hashing trick</em> is often used, where words would be put in a fixed number of <em>buckets</em>, and possibly have overlapping buckets - but as long as the number of buckets is sufficiently large, there's still a good estimation on word distinction.</p>\n<p>Remarks: This is essentially a way to mathematically represent a program that would look in the text for specific mentions of certain words.</p>\n<h3><em>TF-IDF</em>:</h3>\n<p>One problem from one-hot encoding representation is that different words are treated similarly. A lot of times words such as \"is\", \"my\", \"the\" can appear in all sentences, therefore would provide faulty signal (as in, you don't want your model to use the word \"the\" as an indicator of 2 sentences being of different intents).</p>\n<p>One way to address this problem is to remove all words in a list of <em>stop words</em></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> nltk<span class=\"token punctuation\">.</span>corpus <span class=\"token keyword\">import</span> stopwords\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>stopwords<span class=\"token punctuation\">.</span>words<span class=\"token punctuation\">(</span><span class=\"token string\">'english'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>TF-IDF attempts to, instead, count the number of word occurrences, downplaying the more common words and putting more weight on the less common words (stronger signal). This is a step up from one-hot encoding, but is still relatively naive. We will progress extremely fast from here.</p>\n<p>Reference example of these methods: </p>\n<ul>\n<li><a href=\"https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html\">Classification of text documents using sparse features (from scikit-learn documentation)</a></li>\n</ul>\n<h1>2. Word2Vec</h1>\n<p>This paper was a milestone for the use of deep neural networks in NLP. It proposed 2 self-supervised tasks, called <em>\"skip-gram\"</em> and <em>\"continuous bags of words\" (CBOW)</em>, briefly described below:</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Skip-gram</th>\n<th align=\"center\">CBOW</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">Predict surrounding words</td>\n<td align=\"center\">Predict middle word</td>\n</tr>\n<tr>\n<td align=\"center\"><code class=\"language-text\">(&lt;?&gt;,&lt;?&gt;,brown,&lt;?&gt;,&lt;?&gt;) ➞ (the,quick,fox,jumps)</code></td>\n<td align=\"center\"><code class=\"language-text\">(the,quick,&lt;?&gt;,fox, jumps) ➞ brown</code></td>\n</tr>\n</tbody>\n</table>\n<p>With <em>gradient descent</em>, it begins by randomizing - usually a 200-d or 300-d vector - for each words (instead of one-hot encoding), then train for this objective. By the end of training, the resulting word vectors gain an interesting property: the words that are closer in meaning are generally <em>drawn closer</em> - meaning having shorter vector distance. One way to understand this intuitively is similar words tend to be interchangeable in a sentence.\n<img src=\"https://raw.githubusercontent.com/tensorflow/docs/master/site/en/tutorials/text/images/embedding.jpg\" alt=\"Projecting the vectors into 3D space\"></p>\n<p>You can try the <a href=\"http://projector.tensorflow.org/\">Tensorboard Embedding Projector</a> to see what this looks like.</p>\n<p>Another property is that the word vectors can represent some aspects of the meaning, often demonstrated by that we can apply this kind of formula with the resulting word vectors:</p>\n<p><code class=\"language-text\">king - man + woman ~ queen</code> (gender)</p>\n<p>or</p>\n<p><code class=\"language-text\">paris - france + poland ~ warsaw</code> (capital city)</p>\n<p>Naturally this very cherry-picked and is mainly for illustrative purpose and actually doesn't have many applications, but is interesting nontheless.</p>\n<h3>GloVe and FastText</h3>\n<p>GloVe is actually a entirely different formulation altogether (using co-occurence of words) to achieve the same result of <em>drawing similar words closer together</em> in vector space. This resulted in, arguably, the state of the art word vectors for a very long time.</p>\n<p>FastText is a more recent variation of the original Word2Vec formulation: Using the same training objectives, but more performance-optimized training code and more importantly, utilizing n-gram subword features. Specifically, for a 3-gram model, it would break down a word as follows:</p>\n<p><code class=\"language-text\">where ➞ (&lt;wh, whe, her, ere, re&gt;, &lt;where&gt;)</code>\n(note the <code class=\"language-text\">&lt;</code> and <code class=\"language-text\">&gt;</code> markers to denote the beggining and end of the word)</p>\n<p>The final word vector for <code class=\"language-text\">where</code> is then computed as the sum of the modular vectors.</p>\n<p>This method has 2 advantages over the original paper: first, including subword information is helpful, at least for the English language - as e.g <code class=\"language-text\">national</code> and <code class=\"language-text\">nationalism</code> would share a lot of aspects in meaning. Second - this helps dealing with unseen or made-up words, by being able to approximate the meaning if it's based on existing words.</p>\n<p>In practice, FastText results in relatively comparable performance with GloVe, but has the ability to account for unseen words.</p>\n<p>Why I specifically dived into FastText is because <em>breaking down words into subword units</em> would later become a staple for language models to limit vocabulary, which improves computational efficiency, and account for unseen words.</p>\n<p>Reference:</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1301.3781\">Efficient Estimation of Word Representations in Vector Space (2013)</a></li>\n<li><a href=\"https://nlp.stanford.edu/projects/glove/\">GloVe: Global Vectors for Word Representation</a></li>\n<li><a href=\"https://arxiv.org/abs/1607.04606\">Enriching Word Vectors with Subword Information</a></li>\n</ul>","frontmatter":{"title":"A history of progress on text representation in NLP (Part 1 - Word-level representation)","date":"2019-05-27T00:00:00.000Z"},"fields":{"readingTime":{"text":"6 min read"}}}},"pageContext":{"slug":"/blog/2019-05-27-brief-history-of-nlp/"}}}